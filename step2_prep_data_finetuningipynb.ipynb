{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhnanjay/SEC/blob/main/step2_prep_data_finetuningipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Setup"
      ],
      "metadata": {
        "id": "9U3Nc9kGsLS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub datasets --quiet\n",
        "https://twitter.com/virattt/status/1793040762025791870"
      ],
      "metadata": {
        "id": "xsIUPmX46d3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "v1l5EnumGOQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load dataset"
      ],
      "metadata": {
        "id": "nxJcAxq3rjIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "finetuning_dataset = load_dataset(\"virattt/llama-3-8b-financialQA\", split=\"train\")"
      ],
      "metadata": {
        "id": "YCrddtMBug2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvkB_nILsXVM",
        "outputId": "2b5fb3d0-1f98-4073-9096-b84c6ed2b3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'context'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenize the dataset"
      ],
      "metadata": {
        "id": "DPJVrgPas3L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "EW0XCsqArO8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(examples):\n",
        "    # Create the prompt text\n",
        "    text = f\"Question: {examples['question'][0]} Answer: {examples['answer'][0]}\"\n",
        "\n",
        "    # Add pad token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Tokenize the text\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=4096,\n",
        "    )"
      ],
      "metadata": {
        "id": "vnRIBP-VumPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = finetuning_dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "H3bJf6bLrucs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "id": "-riHwGXurysT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create train/test splits"
      ],
      "metadata": {
        "id": "NFc9ZdT1skJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)"
      ],
      "metadata": {
        "id": "Ji7RvDfer2zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(split_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8hCGQ66scmb",
        "outputId": "acc82bb6-4dc9-48f2-d4ef-1d4ca242bc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer', 'context', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 90\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['question', 'answer', 'context', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 10\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    }
  ]
}